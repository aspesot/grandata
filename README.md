# GRANDATA PROJECT


## Resultados de los ejercicios
### Ejercicio 1
- Punto 1: Monto total a facturar: $391367.00
- Punto 2: El dataset en formato parquet pueden encontrarlo en la carpeta output. 
+---------+---------------+--------------------------------+
|id_source|monto_facturado|id_md5                          |
+---------+---------------+--------------------------------+
|4D3      |23403.0        |911914c7729eedbdf5b0f03159f451a9|
|76D      |13664.0        |bd180b7811395cbce5076b52a78ca50d|
|07E      |4226.0         |14a0660ae2f5d186882df86c2972fa93|
|541      |2526.0         |16c222aa19898e5058938167c8ab6c57|
|C25      |2019.0         |0bfa0b57d99985aa138ce05055a3c5db|
|17D      |1209.0         |7521526054bb89ba243073f7e5e541ec|
|3AE      |1087.5         |6a57072949dbc409cc48ef7bd8b05335|
|B86      |1054.5         |bc97b32ee2abb9c18aad55b9da9f51c0|
|E89      |979.5          |5135cc35322269f2fb397b4f81426938|
|B71      |972.0          |1088a10d026eae0ac20f59f5249db2ea|
|162      |887.5          |82aa4b0af34c2313a562076992e50aa3|
|068      |854.5          |fb8490a9504a36c212b5a1be7b4ca7e4|
|A2C      |691.5          |d7dcc6703e425a6e39d6ebaf31cba1bd|
|335      |681.0          |f9b902fc3289af4dd08de5d1de54f68f|
|1BD      |679.0          |f5a45e33602ea62f9498f9d9bb86fc3e|
|3E0      |661.5          |e9d076deb3451cc49819dbc8ed8be7a7|
|76F      |644.0          |1b9204fd05eed7af9ec2ffb0891d84ab|
|6FB      |598.5          |dbcec12ccfc780516eaf452d1d81dd07|
|5D7      |596.5          |d688a78f3638bed0c340ea5ccb972dae|
|84C      |591.5          |f33e3ebc2d9aab9c9a14f2368bec2234|
|FCA      |589.5          |17dfecb54551e0419a4c1b2ba0ee1cf3|
|CF6      |578.0          |3f77e9dfaa037a763ff92fe430985fa4|
|FCF      |554.5          |3e3b743072cab31dc32129a15e816b00|
|0ED      |537.5          |c724fe713b0fc3518eb4e828f0528065|
|CF5      |533.5          |5b8d270f2027a50bf985882a6ff08dc5|
|6A4      |531.0          |5dd9f6f36a6f6d64d887708cf036b450|
|D03      |516.5          |6b9a20a85d8e795d94d47905a75f6ed3|
|FC4      |516.5          |d4e6edd43a77720f39db745caa6dd23b|
|046      |511.0          |bf97f24695f24fac060bc44b4e97acc1|
|C09      |509.5          |166df274636b006e18b32a095829ffdc|
|756      |507.0          |2823f4797102ce1a1aec05359cc16dd9|
|23C      |503.5          |428cdff1010350e01ff31cf9690f27e0|
|08D      |502.5          |f1b82e0bb80be61e2b8ae3a610b9a06d|
|EB1      |498.5          |f0b303b94865daaba2226c8dc0c07353|
|215      |496.0          |3b8a614226a953a8cd9526fca6fe9ba5|
|414      |486.5          |66808e327dc79d135ba18e051673d906|
|897      |468.5          |5705e1164a8394aace6018e27d20d237|
|E98      |464.0          |ccdafd146e9ec355a7ab32313074efa6|
|328      |463.5          |cd00692c3bfe59267d5ecfac5310286c|
|14E      |458.0          |15ca77704847cf3512a011afe8f1ee9a|
|101      |456.0          |38b3eff8baf56627478ec76a704e9b52|
|650      |452.5          |884d247c6f65a96a7da4d1105d584ddd|
|D0B      |448.0          |e87c495061191f4df1ddef45f48bb532|
|E33      |445.5          |43119835da4fc8ef2eaf3337536e78d4|
|F81      |445.5          |0a295ce1395dd390fe66ffd4726fc69e|
|EA0      |444.0          |ea01df294baecce9f9956e4d26c7d5e5|
|280      |443.0          |92c8c96e4c37100777c7190b76d28233|
|E6D      |440.0          |69b2e8cd08123eb60ed65d5257dbd7e3|
|975      |433.0          |92977ae4d2ba21425a59afb269c2a14e|
|249      |428.5          |077e29b11be80ab57e1a2ecabb7da330|
|A4C      |428.5          |24650106de16db2ead5d2b22cdb2f506|
|4B6      |426.5          |74f7fae6893bbd6975ce7525a62705a8|
|BBD      |426.0          |80e01d9926b54fe729410d78954faada|
|683      |424.5          |24681928425f5a9133504de568f5f6df|
|373      |420.5          |ffd52f3c7e12435a724a8f30fddadd9c|
|EB8      |420.0          |c7cac25e467ff56c613d3a58b02e50c9|
|BD4      |418.5          |aa35b36e4d1395542d0da8f9b2138eb3|
|11C      |414.0          |0816da75e13696127a3ca692ccc9d06b|
|26C      |412.5          |e14f1796bb09967d4d815ebbdbc39d85|
|ADC      |412.0          |72c1bba79e6502c017bd14bc00a68491|
|512      |407.0          |10a7cdd970fe135cf4f7bb55c0e3b59f|
|205      |397.0          |eae27d77ca20db309e056e3d2dcd7d69|
|BD8      |395.0          |1bfd138f9868e50335554d0fbe885444|
|98E      |389.0          |e18ae378707d248df94abfe56f03cb46|
|8EF      |384.5          |2e38f1234de642c41f249957d39327d0|
|C82      |384.0          |41fc05922ac4a713bdb2745844c459cd|
|F4B      |383.5          |826893e53afeff983c29371c01586efb|
|CB9      |377.0          |4973b1efa94d92c67c10753c84f281b1|
|857      |376.0          |847cc55b7032108eee6dd897f3bca8a5|
|276      |375.0          |db8e1af0cb3aca1ae2d0018624204529|
|062      |375.0          |18230e1fb6e5dd3cfb0bcec4f863e167|
|E6F      |373.5          |159e1f3d6ff0e7c3c2ff9770edc52cdf|
|370      |371.5          |d709f38ef758b5066ef31b18039b8ce5|
|696      |369.5          |0cb929eae7a499e50248a3a78f7acfc7|
|D7E      |367.0          |3a72ee86a619cc657f1c9f680f3cf0cd|
|ED0      |361.5          |46e7fae94d655842456ba2612f643d46|
|A89      |360.5          |19973e320d023a6ccab786f1fa37c40d|
|F02      |358.0          |c82429e8d9bfeb1caa3ac2f848155553|
|099      |357.0          |8638096e4ddb49a0dd6592c57d9f50ab|
|AC2      |356.0          |6bc3eef723f38424d37d245db8999d89|
|212      |355.5          |1534b76d325a8f591b52d302e7181331|
|A62      |355.0          |c7478a62a4accad1d3fef0cbe6fd67c0|
|4B2      |355.0          |003ad525037042448addfad1371486c3|
|4AD      |351.0          |bed5c94ae8b83526e565cab6c994966f|
|7D8      |350.0          |ecd6be44643bef66ca2b4e8aae5fb3f1|
|1DF      |350.0          |ccc83e5d7011c8df9cc263a2bac0a8f4|
|DFF      |349.5          |bca5fd6b639c7cbf0c78eb9f68d35303|
|DB0      |349.0          |25e64a7f5d1b7073c91470926f61298a|
|D76      |348.5          |ec0a088096d726f7c8e6ddfd8913dd88|
|929      |348.5          |0d0871f0806eae32d30983b62252da50|
|DA5      |346.0          |6a5a0d32c4c51782382bff11d2b894da|
|FD6      |346.0          |a77a18b48cbfc93dfad96164c6a2afec|
|6CB      |345.5          |2921d694e1ab35276513480a8fdeb926|
|054      |344.5          |f75c3dad51560c41ab90ac39560d7a2a|
|EDD      |343.0          |007a277fe5e8059aab75258b67324b35|
|42D      |342.5          |57f2173ca61c078b3ec25e446afaddab|
|2CD      |342.5          |abd3ae089c623abfc73804641f891c65|
|59F      |339.5          |26625083960de6fd20f75b505e27e0ca|
|BCD      |337.0          |8539ef1fba74a70f5a77fcc3f25c1659|
|047      |332.5          |2c4a8495407bfb55ce1a93970445bd38|
+---------+---------------+--------------------------------+
- Punto 3: El histograma se puede encontrar en la carpeta output.
![alt text](output/histograma_llamadas_por_hora.png)

### Ejercicio 2
Si bien no tengo experiencia directa con Hadoop, s√≠ instal√© y trabaj√© con un cl√∫ster Apache Spark en modo standalone sobre tres instancias virtuales. En esa configuraci√≥n defin√≠ un nodo como master y dos como workers, y ejecut√© pipelines desde Airflow hacia ese entorno distribuido. Para responder estas preguntas me bas√© tanto en esa experiencia como en una investigaci√≥n puntual sobre Hadoop y YARN.

#### Pregunta General - 1
Investigu√© c√≥mo funciona YARN y los schedulers que ofrece. Si tuviera que priorizar procesos productivos, usar√≠a colas diferenciadas en el scheduler CapacityScheduler. La cola del pipeline productivo tendr√≠a mayor capacidad m√≠nima, prioridad m√°s alta y posiblemente activar√≠a la opci√≥n de pre-emption si usara FairScheduler, para garantizar que siempre tenga recursos disponibles. Tambi√©n limitar√≠a la cantidad de tareas exploratorias concurrentes para evitar saturar el cluster.

Como estrategia, programar√≠a los procesos productivos en ventanas de baja actividad, como la madrugada, para evitar competir por recursos con tareas exploratorias. Tambi√©n los aislar√≠a temporalmente usando triggers o ventanas de ejecuci√≥n programadas.

Para orquestar esto, tengo experiencia usando Airflow y Dagster que permiten definir dependencias y controlar el momento exacto de ejecuci√≥n de cada job.

#### Pregunta General - 2
Sobre este punto, pensando en mi experiencia pasada veo 2 posibles causas
- Esto se puede dar debido a una mala estrategia de particiones ya que las consultas podr√≠an estar escaneando muchos m√°s datos de los necesarios.
- Bajo rendimiento en la acumulaci√≥n de archivos peque√±os (incluso dentro de las particiones) generados por las actualizaciones diarias. Esto impacta negativamente en la performance porque Spark debe manejar y leer gran cantidad de metadatos y archivos individuales.

Para mejorar esto, sugerir√≠a:
- Aplicar partitioning por una columna como fecha (por ejemplo, fecha_evento) para reducir el volumen de datos escaneados.
- Realizar compactaci√≥n peri√≥dica para reducir small files
- Otra posible soluci√≥n ser√≠a usar Delta Lake, que soporta actualizaciones eficientes, transacciones ACID y permiten aplicar optimizaciones como OPTIMIZE(para compactar) y ZORDER BY (ordenar los registros dentro de la partici√≥n lo que optimiza el corte en la busqueda mas r√°pido cuando encuentra el registro).

- Usar cache() con cuidado en tablas que se consultan mucho y no cambian durante la sesi√≥n.

- Otro soluci√≥n, no tan frecuente, que use en el pasado fue generar una replicaci√≥n actualizada de la tabla con diferentes tipos de particionados armados particularmente para responder a diferentes tipos de acceso a los datos. La √∫nica desventaja de este caso es mantener la consistencia e integridad de los datos ya que el espacio de disco es barato hoy en d√≠a. 

#### Pregunta General - 3 
Una posible configuraci√≥n para cumplir con este punto ser√≠a:
```
spark = SparkSession.builder \
    .appName("ProcesoControlado") \
    .config("spark.executor.instances", "3") \
    .config("spark.executor.memory", "24g") \
    .config("spark.executor.cores", "6") \
    .config("spark.driver.memory", "3g") \
    .getOrCreate()
```
Explicaci√≥n: Para usar solo la mitad del cl√∫ster, calcular√≠a los recursos totales disponibles (150 GB y 36 cores) y luego limitar√≠a el job a 3 ejecutores de 24 GB y 6 cores cada uno. As√≠, uso 72 GB y 18 cores, quedando la otra mitad libre.

Opci√≥n 2: Como vi en la investigaci√≥n previa, usando CapacityScheduler en YARN podria usar una cola con un capacity del 50% y un maximium capacity de 50% asegurandome que no pueda superar el 50% de los recursos.

Otra opci√≥n: Si configuro cada executor con 6 cores y 24 GB de RAM, y habilito dynamic allocation con:

```
.config("spark.dynamicAllocation.enabled", "true")
.config("spark.dynamicAllocation.minExecutors", "1")
.config("spark.dynamicAllocation.maxExecutors", "3")
```
Entonces, el job podr√° escalar din√°micamente, pero nunca va a usar m√°s de 3 executors, lo que me garantiza que el consumo m√°ximo ser√°:
- 3 √ó 6 cores = 18 cores
- 3 √ó 24 GB = 72 GB

Eso representa aproximadamente la mitad del cl√∫ster. Pero esta opci√≥n no es ideal ya que implica reajustar los executor para estas situaciones especificas y es preferible asignar recursos fijos.

## üìÅ Estructura de archivos del proyecto

- /data/ ‚Üí para los archivos eventos.csv.gz y free_sms_destinations.csv.gz

- /notebooks/ ‚Üí Jupyter o Zeppelin notebook

- /output/ ‚Üí parquet y gr√°fico .png

Dockerfile, docker-compose.yml, README.md


## üê≥ Arquitectura Docker

El dise√±o de la soluci√≥n utiliza un √∫nico contenedor definido mediante `docker-compose` y construido con un `Dockerfile` personalizado. Este contenedor incluye:

- Apache Spark 2.3.0 (instalado manualmente)
- Python 3.6
- PySpark 2.3.0
- Jupyter Lab para ejecuci√≥n interactiva

La configuraci√≥n est√° orientada a ejecutarse en modo local (`local[*]`), lo cual es completamente adecuado para los objetivos del ejercicio. Este enfoque evita la complejidad innecesaria de montar un cluster Spark en modo standalone que no aportar√≠a valor adicional para este contexto.

### ‚ñ∂Ô∏è C√≥mo correr el proyecto

1. Clon√° el repositorio y ubic√°te en la ra√≠z del proyecto.

2. Constru√≠ la imagen y levant√° el contenedor:

```bash
docker-compose up --build
```

Luego de la primera instalaci√≥n se puede remover `--build`


