# GRANDATA PROJECT


## Resultados de los ejercicios
### Ejercicio 1
- Punto 1: Monto total a facturar: $391367.00
- Punto 2: El dataset en formato parquet puedes encontrarlo en la carpeta output. Inclu√≠ una sentencia en el codigo para leerlo.
- Punto 3: El histograma se puede encontrar en la carpeta output.

### Ejercicio 2
Si bien no tengo experiencia directa con Hadoop, s√≠ instal√© y trabaj√© con un cl√∫ster Apache Spark en modo standalone sobre tres instancias virtuales. En esa configuraci√≥n defin√≠ un nodo como master y dos como workers, y ejecut√© pipelines desde Airflow hacia ese entorno distribuido. Para responder estas preguntas me bas√© tanto en esa experiencia como en una investigaci√≥n puntual sobre Hadoop y YARN.

--------------- Punto 1 -----------------
Investigu√© c√≥mo funciona YARN y los schedulers que ofrece. Si tuviera que priorizar procesos productivos, usar√≠a colas diferenciadas en el scheduler CapacityScheduler. La cola del pipeline productivo tendr√≠a mayor capacidad m√≠nima, prioridad m√°s alta y posiblemente activar√≠a la opci√≥n de pre-emption si usara FairScheduler, para garantizar que siempre tenga recursos disponibles. Tambi√©n limitar√≠a la cantidad de tareas exploratorias concurrentes para evitar saturar el cluster.

Como estrategia, programar√≠a los procesos productivos en ventanas de baja actividad, como la madrugada, para evitar competir por recursos con tareas exploratorias. Tambi√©n los aislar√≠a temporalmente usando triggers o ventanas de ejecuci√≥n programadas.

Para orquestar esto, tengo experiencia usando Airflow y Dagster que permiten definir dependencias y controlar el momento exacto de ejecuci√≥n de cada job.

--------------- Punto 2 -----------------
Sobre este punto, pensando en mi experiencia pasada veo 2 posibles causas
- Esto se puede dar debido a una mala estrategia de particiones ya que las consultas podr√≠an estar escaneando muchos m√°s datos de los necesarios.
- Bajo rendimiento en la acumulaci√≥n de archivos peque√±os (incluso dentro de las particiones) generados por las actualizaciones diarias. Esto impacta negativamente en la performance porque Spark debe manejar y leer gran cantidad de metadatos y archivos individuales.

Para mejorar esto, sugerir√≠a:
- Aplicar partitioning por una columna como fecha (por ejemplo, fecha_evento) para reducir el volumen de datos escaneados.
- Realizar compactaci√≥n peri√≥dica para reducir small files
- Otra posible soluci√≥n ser√≠a usar Delta Lake, que soporta actualizaciones eficientes, transacciones ACID y permiten aplicar optimizaciones como OPTIMIZE(para compactar) y ZORDER BY (ordenar los registros dentro de la partici√≥n lo que optimiza el corte en la busqueda mas r√°pido cuando encuentra el registro).

- Usar cache() con cuidado en tablas que se consultan mucho y no cambian durante la sesi√≥n.

- Otro soluci√≥n, no tan frecuente, que use en el pasado fue generar una replicaci√≥n actualizada de la tabla con diferentes tipos de particionados armados particularmente para responder a diferentes tipos de acceso a los datos. La √∫nica desventaja de este caso es mantener la consistencia e integridad de los datos ya que el espacio de disco es barato hoy en d√≠a. 

--------------- Punto 3 -----------------
Una posible configuraci√≥n para cumplir con este punto ser√≠a:
```
spark = SparkSession.builder \
    .appName("ProcesoControlado") \
    .config("spark.executor.instances", "3") \
    .config("spark.executor.memory", "24g") \
    .config("spark.executor.cores", "6") \
    .config("spark.driver.memory", "3g") \
    .getOrCreate()
```
Explicaci√≥n: Para usar solo la mitad del cl√∫ster, calcular√≠a los recursos totales disponibles (150 GB y 36 cores) y luego limitar√≠a el job a 3 ejecutores de 24 GB y 6 cores cada uno. As√≠, uso 72 GB y 18 cores, quedando la otra mitad libre.

Opci√≥n 2: Como vi en la investigaci√≥n previa, usando CapacityScheduler en YARN podria usar una cola con un capacity del 50% y un maximium capacity de 50% asegurandome que no pueda superar el 50% de los recursos.

Otra opci√≥n: Si configuro cada executor con 6 cores y 24 GB de RAM, y habilito dynamic allocation con:

```
.config("spark.dynamicAllocation.enabled", "true")
.config("spark.dynamicAllocation.minExecutors", "1")
.config("spark.dynamicAllocation.maxExecutors", "3")
```
Entonces, el job podr√° escalar din√°micamente, pero nunca va a usar m√°s de 3 executors, lo que me garantiza que el consumo m√°ximo ser√°:
- 3 √ó 6 cores = 18 cores
- 3 √ó 24 GB = 72 GB

Eso representa aproximadamente la mitad del cl√∫ster. Pero esta opci√≥n no es ideal ya que implica reajustar los executor para estas situaciones especificas y es preferible asignar recursos fijos.

## üìÅ Estructura de archivos del proyecto

- /data/ ‚Üí para los archivos eventos.csv.gz y free_sms_destinations.csv.gz

- /notebooks/ ‚Üí Jupyter o Zeppelin notebook

- /output/ ‚Üí parquet y gr√°fico .png

Dockerfile, docker-compose.yml, README.md


## üê≥ Arquitectura Docker

El dise√±o de la soluci√≥n utiliza un √∫nico contenedor definido mediante `docker-compose` y construido con un `Dockerfile` personalizado. Este contenedor incluye:

- Apache Spark 2.3.0 (instalado manualmente)
- Python 3.6
- PySpark 2.3.0
- Jupyter Lab para ejecuci√≥n interactiva

La configuraci√≥n est√° orientada a ejecutarse en modo local (`local[*]`), lo cual es completamente adecuado para los objetivos del ejercicio. Este enfoque evita la complejidad innecesaria de montar un cluster Spark en modo standalone que no aportar√≠a valor adicional para este contexto.

### ‚ñ∂Ô∏è C√≥mo correr el proyecto

1. Clon√° el repositorio y ubic√°te en la ra√≠z del proyecto.

2. Constru√≠ la imagen y levant√° el contenedor:

```bash
docker-compose up --build
```

Luego de la primera instalaci√≥n se puede remover `--build`


